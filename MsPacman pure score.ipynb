{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"MsPacman pure score.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"jKkF3W_to_JH","colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"status":"ok","timestamp":1622039500664,"user_tz":-480,"elapsed":23,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}},"outputId":"95473ce6-6d13-447d-d826-ed4f0e0f2ffd"},"source":["'''\n","# Initialise RAM Usage\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n","'''"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# Initialise RAM Usage\\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\\n!pip install gputil\\n!pip install psutil\\n!pip install humanize\\n\\nimport psutil\\nimport humanize\\nimport os\\nimport GPUtil as GPU\\nGPUs = GPU.getGPUs()\\n# XXX: only one GPU on Colab and isn’t guaranteed\\ngpu = GPUs[0]\\ndef printm():\\n process = psutil.Process(os.getpid())\\n print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\\n print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\\nprintm()\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YelGIaK8fcQf","executionInfo":{"status":"ok","timestamp":1622039501928,"user_tz":-480,"elapsed":1282,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}},"outputId":"34a9b08e-c2ab-4394-babb-53b70fb6a594"},"source":["#!git clone https://github.com/James-LLJ/gym\n","!git clone https://github.com/James-LLJ/Atari-MsPacman-v0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'Atari-MsPacman-v0'...\n","remote: Enumerating objects: 10960, done.\u001b[K\n","remote: Counting objects: 100% (10960/10960), done.\u001b[K\n","remote: Compressing objects: 100% (2848/2848), done.\u001b[K\n","remote: Total 10960 (delta 7997), reused 10880 (delta 7947), pack-reused 0\u001b[K\n","Receiving objects: 100% (10960/10960), 3.06 MiB | 10.72 MiB/s, done.\n","Resolving deltas: 100% (7997/7997), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Js_g-cysnhS1","executionInfo":{"status":"ok","timestamp":1622039506994,"user_tz":-480,"elapsed":5070,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}},"outputId":"dcd0748c-db23-4de6-c194-b929f83a21e6"},"source":["%cd ./Atari-MsPacman-v0\n","!pip install -e."],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/Atari-MsPacman-v0\n","Obtaining file:///content/Atari-MsPacman-v0\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.18.0) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.18.0) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.18.0) (1.5.0)\n","Requirement already satisfied: Pillow<=7.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.18.0) (7.1.2)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.18.0) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.18.0) (0.16.0)\n","Installing collected packages: gym\n","  Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","  Running setup.py develop for gym\n","Successfully installed gym\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mVkqe0f6pXIm","executionInfo":{"status":"ok","timestamp":1622039509568,"user_tz":-480,"elapsed":2579,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow.compat.v1 as tf\n","import numpy as np\n","import pandas as pd\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import os\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","from __future__ import absolute_import, division, print_function\n","from tensorflow import keras\n","\n","import csv\n","from google.colab import files"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_C2vD2bnEQy"},"source":["## Task 1: Processing game image \n","\n","Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n","\n","We can thus save a lot of time by preprocessing game image, including\n","* Resizing to a smaller shape, 64 x 64\n","* Converting to grayscale\n","* Cropping irrelevant image parts (top & bottom)"]},{"cell_type":"code","metadata":{"id":"h6WwYJSqnEQz","executionInfo":{"status":"ok","timestamp":1622039509569,"user_tz":-480,"elapsed":12,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["def rgb2gray(rgb):\n","    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2DZvS8RnEQ1","executionInfo":{"status":"ok","timestamp":1622039510096,"user_tz":-480,"elapsed":536,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["from gym.core import ObservationWrapper\n","from gym.spaces import Box\n","import skimage\n","\n","#from scipy.misc import imresize\n","from skimage import color\n","from skimage import transform\n","\n","class PreprocessAtari(ObservationWrapper):\n","    def __init__(self, env):\n","        \"\"\"A gym wrapper that crops, scales image into the desired shapes and optionally grayscales it.\"\"\"\n","        ObservationWrapper.__init__(self,env)\n","        \n","        self.img_size = (64, 64)\n","        self.observation_space = Box(0.0, 1.0, shape=(self.img_size[0], self.img_size[1], 1))\n","\n","    def observation(self, img):\n","        \"\"\"what happens to each observation\"\"\"\n","        \n","        # Here's what you need to do:\n","        #  * crop image, remove irrelevant parts\n","        #  * resize image to self.img_size \n","        #     (use imresize imported above or any library you want,\n","        #      e.g. opencv, skimage, PIL, keras)\n","        #  * cast image to grayscale\n","        #  * convert image pixels to (0,1) range, float32 type\n","\n","        top = 10\n","        bottom = len(img)-40\n","        left = 5\n","        right = len(img[1])-5\n","        crop_img = img[top:bottom, left:right, :]\n","        #print(top, bottom, left, right)\n","\n","        img2 = skimage.transform.resize(crop_img, self.img_size)\n","        img2 = color.rgb2gray(img2)\n","        s = (img2.shape[0], img2.shape[1], 1 )\n","        img2 = img2.reshape(s)\n","        img2 = img2.astype('float32') / img2.max()\n","        return img2\n","  \n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":902},"id":"nd5ZBywmnEQ3","executionInfo":{"status":"error","timestamp":1622039511287,"user_tz":-480,"elapsed":1198,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}},"outputId":"825351b0-9704-491d-9525-99d8028d31af"},"source":["#spawn game instance for tests\n","#env = gym.make(\"Alien-v0\")\n","#env = PreprocessAtari(env)\n","\n","# Run MsPacman-v0 for testing\n","env = gym.make(\"MsPacman-v0\")\n","\n","\n","observation_shape = env.observation_space.shape\n","n_actions = env.action_space.n\n","\n","obs = env.reset()\n","print(obs.shape)\n","print(n_actions)\n","\n","#test observation\n","assert obs.ndim == 3, \"observation must be [batch, time, channels] even if there's just one channel\"\n","assert obs.shape == observation_shape\n","#assert obs.dtype == 'float32'\n","#assert len(np.unique(obs))>2, \"your image must not be binary\"\n","#assert 0 <= np.min(obs) and np.max(obs) <=1, \"convert image pixels to (0,1) range\"\n","\n","print(\"Formal tests seem fine. Here's an example of what you'll get.\")\n","\n","plt.title(\"what your network gonna see\")\n","plt.imshow(obs[:, :, 0], interpolation='none');\n"],"execution_count":7,"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-69f51e7caa63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run MsPacman-v0 for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacman-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/Atari-MsPacman-v0/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/Atari-MsPacman-v0/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/Atari-MsPacman-v0/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Make the environment aware of which spec it came from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/Atari-MsPacman-v0/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_game_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_difficulty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifficulty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/atari_py/games.py\u001b[0m in \u001b[0;36mget_game_path\u001b[0;34m(game_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_games_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ROM is missing for %s, see https://github.com/openai/atari-py#roms for instructions'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: ROM is missing for ms_pacman, see https://github.com/openai/atari-py#roms for instructions"]}]},{"cell_type":"markdown","metadata":{"id":"kHYmVpCSnEQ5"},"source":["## Frame buffer\n","\n","Our agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity.\n","\n","To do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you."]},{"cell_type":"code","metadata":{"id":"Os6le8Afqi3_","executionInfo":{"status":"aborted","timestamp":1622039511242,"user_tz":-480,"elapsed":29,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["from gym.core import Wrapper\n","class FrameBuffer(Wrapper):\n","    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n","        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n","        super(FrameBuffer, self).__init__(env)\n","        self.dim_order = dim_order\n","        if dim_order == 'tensorflow':\n","            height, width, n_channels = env.observation_space.shape # h=64, width=64, channels=1\n","            obs_shape = [height, width, n_channels * n_frames]\n","        elif dim_order == 'pytorch':\n","            n_channels, height, width = env.observation_space.shape\n","            obs_shape = [n_channels * n_frames, height, width]\n","        else:\n","            raise ValueError('dim_order should be \"tensorflow\" or \"pytorch\", got {}'.format(dim_order))\n","        self.observation_space = Box(0.0, 1.0, obs_shape)\n","        self.framebuffer = np.zeros(obs_shape, 'float32')\n","        \n","    def reset(self):\n","        \"\"\"resets breakout, returns initial frames\"\"\"\n","        self.framebuffer = np.zeros_like(self.framebuffer)\n","        self.update_buffer(self.env.reset())\n","        return self.framebuffer\n","    \n","    def step(self, action):\n","        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n","        minAlien, minDist, new_img, score, reward, done, info = self.env.step(action)\n","        self.update_buffer(new_img)\n","        #minAlien = 111\n","        #minDist = 222\n","        return  minAlien, minDist, self.framebuffer, score, reward, done, info\n","    \n","    def update_buffer(self, img):\n","        if self.dim_order == 'tensorflow':\n","            offset = self.env.observation_space.shape[-1]\n","            axis = -1\n","            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n","        elif self.dim_order == 'pytorch':\n","            offset = self.env.observation_space.shape[0]\n","            axis = 0\n","            cropped_framebuffer = self.framebuffer[:-offset]\n","        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmo_SpKonEQ6","scrolled":false,"executionInfo":{"status":"aborted","timestamp":1622039511243,"user_tz":-480,"elapsed":29,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["def make_env():\n","    env = gym.make(\"MsPacman-v0\")\n","  \n","    #env = PreprocessAtari(env)\n","    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n","    return env\n","\n","env = make_env()\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbWLlyk-nEQ-"},"source":["## Task 2: Building a network\n","\n","We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be reseted unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n"]},{"cell_type":"markdown","metadata":{"id":"SMhmDKmjnEQ_"},"source":["![img](https://github.com/shenweichen/Coursera/blob/master/Specialization_Advanced_Machine_Learning_Higher_School_of_Economics/Course4_Practical%20Reinforcement%20Learning/week4_approx/img/cnn.png?raw=1)"]},{"cell_type":"code","metadata":{"id":"pbw8DFQjnERA","executionInfo":{"status":"aborted","timestamp":1622039511244,"user_tz":-480,"elapsed":30,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["try:\n","    sess.close()\n","except:\n","    pass\n","\n","tf.reset_default_graph()\n","sess = tf.InteractiveSession()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQVgT0_BnERD","executionInfo":{"status":"aborted","timestamp":1622039511246,"user_tz":-480,"elapsed":32,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["from keras.layers import Conv2D, Dense, Flatten\n","from keras.models import Sequential\n","tf.compat.v1.disable_eager_execution()\n","\n","# Define a variable for epsilon of each step. epsilon_display will be displayed every epsidode\n","epsilon_display = 0\n","\n","class DQNAgent:\n","    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False, Eps_start=1.0000, Eps_end=0.100, Eps_decay=0.9971):\n","        \"\"\"A simple DQN agent\"\"\"\n","        with tf.compat.v1.variable_scope(name, reuse=reuse):\n","            \n","            #< Define your network body here. Please make sure you don't use any layers created elsewhere >\n","            self.network = Sequential()\n","            self.network.add(Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), activation='relu'))\n","            self.network.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), activation='relu'))\n","            self.network.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), activation='relu'))\n","            self.network.add(Flatten())\n","            self.network.add(Dense(256, activation='relu'))\n","            self.network.add(Dense(n_actions, activation='linear'))\n","            \n","            # prepare a graph for agent step            \n","            self.state_t = tf.placeholder('float32', [None,] + list(state_shape),name='self.state_t')\n","            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n","            \n","        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n","        self.Eps_start = Eps_start\n","        self.Eps_end = Eps_end\n","        self.Eps_decay = Eps_decay\n","\n","    def get_symbolic_qvalues(self, state_t):\n","        \"\"\"takes agent's observation, returns qvalues. Both are tf Tensors\"\"\"\n","       \n","        qvalues = self.network(state_t) #< symbolic tensor for q-values >\n","        \n","        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n","            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n","        assert int(qvalues.shape[1]) == n_actions\n","        \n","        return qvalues\n","    \n","    def get_qvalues(self, state_t):\n","        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n","        sess = tf.get_default_session()\n","        return sess.run(self.qvalues_t, {self.state_t: state_t})\n","    \n","    def sample_actions(self, qvalues, episode):\n","        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n","        batch_size, n_actions = qvalues.shape\n","        random_actions = np.random.choice(n_actions, size=batch_size)\n","        best_actions = qvalues.argmax(axis=-1)\n","\n","        self.explore_epis = 1400  # Agent only explores for 'explore_epis' episodes\n","\n","        self.Eps_decay_exp = (self.Eps_end / self.Eps_start)**(1/self.explore_epis)\n","        epsilon = max(self.Eps_end, round(self.Eps_start * self.Eps_decay_exp**episode, 4)) # Exponential decay\n","        # self.Eps_decay_lin = (self.Eps_start - self.Eps_end)/self.explore_epis\n","        # epsilon =  max(self.Eps_end, round(self.Eps_start - episode*self.Eps_decay_lin, 4)) # Linear decay based on episode  \n","\n","        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon]) # 1 for random action and 0 for best action\n","        return np.where(should_explore, random_actions, best_actions)[0], epsilon # More 1 in should_explore, more exploitation and tend to choose best_actions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyzjFxIWKoQs","executionInfo":{"status":"aborted","timestamp":1622039511247,"user_tz":-480,"elapsed":32,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["epsilon_linear_arr = []\n","epi_explore = 100\n","for episode in range(150):\n","    epsilon = max(0.1, 1 - (1 - 0.1)/epi_explore * episode)\n","    epsilon_linear_arr.append(round(epsilon,4))\n","\n","print(epsilon_linear_arr)\n","\n","epsilon_expo_arr = []\n","eps_explore_expo = 100\n","for episode in range(150):\n","    eps_decay = (0.1/1)**(1/eps_explore_expo)\n","    epsilon = max(0.1, 1 * eps_decay**episode)\n","    epsilon_expo_arr.append(round(epsilon,4))\n","\n","print(epsilon_expo_arr)\n","\n","plt.plot(epsilon_linear_arr)\n","plt.plot(epsilon_expo_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6YAeaoinERF","executionInfo":{"status":"aborted","timestamp":1622039511248,"user_tz":-480,"elapsed":33,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["agent = DQNAgent(\"dqn_agent\", state_dim, n_actions)\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XWEZumNnERI"},"source":["Now let's try out our agent to see if it raises any errors."]},{"cell_type":"markdown","metadata":{"id":"aQdG1ceenERN"},"source":["## Experience replay\n","You are also provided with experience replay buffer as follows.\n","\n","![img](https://github.com/shenweichen/Coursera/blob/master/Specialization_Advanced_Machine_Learning_Higher_School_of_Economics/Course4_Practical%20Reinforcement%20Learning/week4_approx/img/exp_rep.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"MN4fbcJAnERO"},"source":["#### The interface is fairly simple:\n","* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n","* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n","* `len(exp_replay)` - returns number of elements stored in replay buffer."]},{"cell_type":"code","metadata":{"id":"9kToHWFOvKul","executionInfo":{"status":"aborted","timestamp":1622039511250,"user_tz":-480,"elapsed":35,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["import numpy as np\n","import random\n","\n","class ReplayBuffer(object):\n","    def __init__(self, size):\n","        \"\"\"Create Replay buffer.\n","        Parameters\n","        ----------\n","        size: int\n","            Max number of transitions to store in the buffer. When the buffer\n","            overflows, the old memories are dropped.\n","        \"\"\"\n","        self._storage = []\n","        self._maxsize = size\n","        self._next_idx = 0\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def add(self, obs_t, action, reward, obs_tp1, done):\n","        data = (obs_t, action, reward, obs_tp1, done)\n","\n","        if self._next_idx >= len(self._storage):\n","            self._storage.append(data)\n","        else:\n","            self._storage[self._next_idx] = data\n","        self._next_idx = (self._next_idx + 1) % self._maxsize\n","\n","    def _encode_sample(self, idxes):\n","        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n","        for i in idxes:\n","            data = self._storage[i]\n","            obs_t, action, reward, obs_tp1, done = data\n","            obses_t.append(np.array(obs_t, copy=False))\n","            actions.append(np.array(action, copy=False))\n","            rewards.append(reward)\n","            obses_tp1.append(np.array(obs_tp1, copy=False))\n","            dones.append(done)\n","        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch of experiences.\n","        Parameters\n","        ----------\n","        batch_size: int\n","            How many transitions to sample.\n","        Returns\n","        -------\n","        obs_batch: np.array\n","            batch of observations\n","        act_batch: np.array\n","            batch of actions executed given obs_batch\n","        rew_batch: np.array\n","            rewards received as results of executing act_batch\n","        next_obs_batch: np.array\n","            next set of observations seen after executing act_batch\n","        done_mask: np.array\n","            done_mask[i] = 1 if executing act_batch[i] resulted in\n","            the end of an episode and 0 otherwise.\n","        \"\"\"\n","        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n","        return self._encode_sample(idxes)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hRQKSHE72f7p"},"source":["## Let's play the game!"]},{"cell_type":"code","metadata":{"id":"5IMviLDVnERS","executionInfo":{"status":"aborted","timestamp":1622039511251,"user_tz":-480,"elapsed":35,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["\n","def play_and_record(agent, env, exp_replay, n_steps, episode, train_step=None, td_loss=None):\n","    \"\"\"\n","    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n","    Whenever game ends, add record with done=True and reset the game.\n","    :returns: return sum of rewards over time\n","    \n","    Note: please do not env.reset() unless env is done.\n","    It is guaranteed that env has done=False when passed to this function.\n","    \"\"\"\n","\n","    minAlienmat=[]\n","    minDistmat=[]\n","    minAlienmat_avg = 0\n","    minDistmat_avg = 0\n","\n","    # State at the beginning of rollout\n","    s = env.framebuffer\n","    \n","    # Play the game for n_steps as per instructions above\n","    reward = 0\n","    epsilon = None\n","    num_step = 0\n","    score_per_epi = 0\n","\n","    for t in range(n_steps):\n","\n","        qvalues = agent.get_qvalues([s])\n","        action, epsilon = agent.sample_actions(qvalues, episode)\n","        minAlien, minDist, next_s, score, r, done, info = env.step(action)\n","        # exp_replay.add(s, action, score, next_s, done)  # Change this line to train agent with game score / EM reward\n","        exp_replay.add(s, action, score, next_s, done)  # Train agent with a combination of game score and EM reward. \n","                                                            # We need to set ratio of game score : emotional reward\n","        score_per_epi += score\n","        reward += r        \n","\n","        #Since the the default value for both distances is 1000 need the if condition. Appends the distances in an array to find avg per episode later. \n","        if minAlien<1000:\n","          minAlienmat=np.append(minAlienmat,minAlien)\n","\n","        if minDist<1000:\n","          minDistmat=np.append(minDistmat,minDist)\n","\n","        # train\n","        if (train_step is not None and (t+1) % 4 == 0):  # Change the frequency of optimising the TD loss \n","            _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n","            td_loss_history.append(loss_t)  \n","        \n","\n","        if done:  \n","\n","            # Print minAlien and minDist per game\n","            minAlienmat_avg = np.mean(minAlienmat)   \n","            minDistmat_avg = np.mean(minDistmat)\n","            # print(minAlienmat)\n","            # print(minDistmat)\n","            print(\"Min alien\", minAlienmat_avg)\n","            print(\"Min distance\", minDistmat_avg)\n","\n","            s = env.reset() \n","            num_step = t\n","            #print(\"DEAD!!\", \"Reward is:  \", reward, \"   Epsilon is:  \", epsilon_display)\n","            break\n","            \n","        else:\n","            s = next_s\n","        \n","    return reward, epsilon, num_step, minAlienmat_avg, minDistmat_avg, score_per_epi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CBnbfswnERZ"},"source":["### Target networks\n","\n","We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n","\n","The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n","\n","$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n","\n","![img](https://github.com/shenweichen/Coursera/blob/master/Specialization_Advanced_Machine_Learning_Higher_School_of_Economics/Course4_Practical%20Reinforcement%20Learning/week4_approx/img/target.png?raw=1)\n","\n"]},{"cell_type":"code","metadata":{"id":"gRzsjZrhnERa","executionInfo":{"status":"aborted","timestamp":1622039511252,"user_tz":-480,"elapsed":36,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["target_network = DQNAgent(\"target_network\", state_dim, n_actions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2jHssZWnERk","executionInfo":{"status":"aborted","timestamp":1622039511253,"user_tz":-480,"elapsed":37,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["def load_weigths_into_target_network(agent, target_network):\n","    \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n","    assigns = []\n","    for w_agent, w_target in zip(agent.weights, target_network.weights):\n","        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n","    tf.get_default_session().run(assigns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zno4rQxPnER1"},"source":["### Learning with... Q-learning\n","Here we write a function similar to `agent.update` from tabular Q-learning (last exercise). "]},{"cell_type":"code","metadata":{"id":"feieN0z9nER2","executionInfo":{"status":"aborted","timestamp":1622039511257,"user_tz":-480,"elapsed":40,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# placeholders that will be fed with exp_replay.sample(batch_size)\n","obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim,name='obs')\n","actions_ph = tf.placeholder(tf.int32, shape=[None],name='actions')\n","rewards_ph = tf.placeholder(tf.float32, shape=[None],name='rewards')\n","next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim,name='next_obs')\n","is_done_ph = tf.placeholder(tf.float32, shape=[None],name='is_done')\n","\n","is_not_done = 1 - is_done_ph\n","gamma = 0.99"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vq1aFNiTnER4"},"source":["Take q-values for actions agent just took"]},{"cell_type":"code","metadata":{"id":"FqA4OCU6nER5","executionInfo":{"status":"aborted","timestamp":1622039511258,"user_tz":-480,"elapsed":41,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n","current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tB0UWxDnER8"},"source":["Compute Q-learning TD error:\n","\n","$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n","\n","With Q-reference defined as\n","\n","$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n","\n","Where\n","* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n","* $s, a, r, s'$ are current state, action, reward and next state respectively\n","* $\\gamma$ is a discount factor defined two cells above."]},{"cell_type":"code","metadata":{"id":"gocVVHTEYwxH","executionInfo":{"status":"aborted","timestamp":1622039511260,"user_tz":-480,"elapsed":43,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# compute q-values for NEXT states with target network\n","next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph)\n","\n","# compute state values by taking max over next_qvalues_target for all actions\n","next_state_values_target =  is_not_done * tf.reduce_max(next_qvalues_target,axis=1)\n","\n","# compute Q_reference(s,a) as per formula above.\n","reference_qvalues = rewards_ph +   gamma * next_state_values_target\n","\n","# Define loss function for sgd.\n","td_loss = (current_action_qvalues - reference_qvalues) ** 2\n","td_loss = tf.reduce_mean(td_loss)\n","\n","train_step = tf.train.AdamOptimizer(1e-3).minimize(td_loss, var_list=agent.weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpWfuZ5WnESA","executionInfo":{"status":"aborted","timestamp":1622039511262,"user_tz":-480,"elapsed":45,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uH1zMW5snESJ"},"source":["### Main loop\n","\n","It's time to put everything together and see if it learns anything. Training anything meaningful will take a long time. **It may even be too long for the Colab runtime even with the GPU.** We will cut the training short in this exercise just to demonstrate the idea."]},{"cell_type":"code","metadata":{"id":"P_56wlE0nESK","executionInfo":{"status":"aborted","timestamp":1622039511264,"user_tz":-480,"elapsed":46,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["from tqdm import trange\n","from IPython.display import clear_output\n","#from pandas import ewma\n","#import pandas as pd\n","%matplotlib inline\n","\n","plt.rcParams[\"figure.figsize\"] = (8, 16)\n","\n","mean_rw_history = []\n","td_loss_history = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"beOpHSB1nESL","executionInfo":{"status":"aborted","timestamp":1622039511265,"user_tz":-480,"elapsed":47,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["exp_replay = ReplayBuffer(10000)\n","\n","def sample_batch(exp_replay, batch_size):\n","    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n","    return {\n","        obs_ph:obs_batch, actions_ph:act_batch, rewards_ph:reward_batch, \n","        next_obs_ph:next_obs_batch, is_done_ph:is_done_batch\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pN9RzZRYfWY","executionInfo":{"status":"aborted","timestamp":1622039511266,"user_tz":-480,"elapsed":48,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pj8LeOUraCpj","executionInfo":{"status":"aborted","timestamp":1622039511267,"user_tz":-480,"elapsed":49,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# !pip install pandas\n","import pandas as pd\n","pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Pure score.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXhfPVdO5sXW","executionInfo":{"status":"aborted","timestamp":1622039511268,"user_tz":-480,"elapsed":50,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# Create a function to save checkpoints and a function to load checkpoints from Google Drive\n","\n","def save_checkpoint():\n","    agent.network.save('/content/gdrive/My Drive/Colab Notebooks/Agent network_pure score')\n","    target_network.network.save('/content/gdrive/My Drive/Colab Notebooks/Target network_pure score')\n","    agent.network.summary()\n","    target_network.network.summary()\n","\n","def load_checkpoint():\n","    global agent, target_network\n","    agent.network = keras.models.load_model('/content/gdrive/My Drive/Colab Notebooks/Agent network_pure score', compile=False)  \n","    target_network.target_network = keras.models.load_model('/content/gdrive/My Drive/Colab Notebooks/Target network_pure score', compile=False)\n","    agent.network.summary()\n","    target_network.network.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UkheLmZk3r8","executionInfo":{"status":"aborted","timestamp":1622039511269,"user_tz":-480,"elapsed":50,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# Set a Boolean value to check if we need to load models from Google Drive\n","\n","#check_load_model = False\n","#check_load_model = True\n","check_load_model = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEBW0cPwSc4v","executionInfo":{"status":"aborted","timestamp":1622039511270,"user_tz":-480,"elapsed":51,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["episode_array = []\n","epsilon_array = []\n","num_steps = []\n","rewards = []\n","gamescores = []\n","minAlien_arr = []\n","minDist_arr = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sju50qREnESN","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1622039511272,"user_tz":-480,"elapsed":53,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["if check_load_model == False:\n","       \n","    for episode in range(1500):\n","        # play\n","        # reward, epsilon, num_step, minAlienmat_avg, minDistmat_avg, score_per_epi\n","        reward, epsilon, steps, minAlienmat_avg, minDistmat_avg, score = play_and_record(agent, env, exp_replay, n_steps=10000,\n","                                                episode=episode, train_step=train_step, td_loss=td_loss)\n","        #print(\"Alien Distance:  \", minAlienmat_avg, \"; Egg Distance:  \", minDistmat_avg)\n","        # Store epsilon, scores and steps of each episode in epsilon_array\n","        epsilon_array.append(epsilon)\n","        num_steps.append(steps)\n","        rewards.append(reward)\n","        gamescores.append(score)\n","        minAlien_arr.append(minAlienmat_avg)\n","        minDist_arr.append(minDistmat_avg)\n","        \n","        # Create a CSV file to save output values. Save it and save training models every 10 episode. \n","        fileName = '/content/gdrive/My Drive/Colab Notebooks/Pure score.csv'\n","\n","        if episode % 5 == 0:\n","            save_checkpoint()\n","\n","            with open(fileName, mode='w') as csv_file:\n","                fieldnames = ['Episode', 'Epsilon', 'Step', 'GameScore', 'Reward', 'MinAlien', 'MinDist']\n","                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","                writer.writeheader()\n","\n","                for i in range(episode):\n","                    writer.writerow({'Episode': i, 'Epsilon': epsilon_array[i],\n","                                    'Step': num_steps[i], 'GameScore': gamescores[i], 'Reward': rewards[i], \n","                                     'MinAlien': minAlien_arr[i], 'MinDist': minDist_arr[i]})\n","\n","        # Adjust agent parameters by loading policy network into target network\n","        if episode % 30 == 0 and episode != 0:\n","            load_weigths_into_target_network(agent, target_network)\n","            print(\"Load nerual network weights from Policy Network to Target Nework\")\n","\n","            if np.mean(gamescores[-10:]) > 800:\n","                print('Should be ok')\n","                break\n","                \n","        print(\"episode = %i, buffer size = %i, epsilon = %.5f\" % (episode, len(exp_replay), epsilon))\n","        clear_output(wait=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMguDRtH5sXW","executionInfo":{"status":"aborted","timestamp":1622039511274,"user_tz":-480,"elapsed":55,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# Create a function to load and draw saved values\n","def load_and_draw(): \n","\n","    temp_episode = []\n","    temp_epsilon = []\n","    temp_step = []\n","    temp_score = []\n","    temp_reward = []\n","    temp_minAlien = []\n","    temp_minDist = []\n","\n","    score_avg = []\n","    reward_avg = []\n","\n","    with open('/content/gdrive/My Drive/Colab Notebooks/Pure score.csv', mode='r') as csv_file:\n","        csv_reader = csv.DictReader(csv_file)\n","        line_count = 0 \n","\n","        for row in csv_reader:\n","            if line_count == 0:\n","                print(f'Column names are {\", \".join(row)}')\n","                line_count += 1\n","                \n","            # print(f'Episode is {row[\"Epsilon\"]}, Step is {row[\"Steps\"]}, and Score is {row[\"Game_scores\"]}.')\n","\n","            line_count += 1\n","            temp_episode.append(int(row['Episode']))\n","            temp_epsilon.append(float(row['Epsilon']))\n","            temp_step.append(int(row['Step']))\n","            temp_score.append(float(row['GameScore']))\n","            temp_reward.append(float(row['Reward']))\n","            temp_minAlien.append(float(row['MinAlien']))\n","            temp_minDist.append(float(row['MinDist']))\n","\n","    plt.subplot(4,1,1)\n","    plt.title(\"Epsilon changes over time\")\n","    plt.ylabel(\"Epsilon\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_epsilon)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Plot trend of number of steps\n","    plt.subplot(4,1,2)\n","    plt.title(\"Number of steps per episode\")\n","    plt.ylabel(\"Number of steps\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_step)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Plot trend of gamescore\n","    plt.subplot(4,1,3)\n","    plt.title('Game score per episode')\n","    plt.ylabel(\"Game score\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_score)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Plot trend of emotional reward\n","    plt.subplot(4,1,4)\n","    plt.title('Emotional reward per episode')\n","    plt.ylabel(\"Reward\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_reward)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Plot trend of minimum distance to an alien \n","    plt.subplot(4,1,1)\n","    plt.title('Minimum distance to an alien per episode')\n","    plt.ylabel(\"Distance\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_minAlien)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Plot trend of minimum distance to a point\n","    plt.subplot(4,1,2)\n","    plt.title('Minimum distance to a point per episode')\n","    plt.ylabel(\"Distance\")\n","    plt.xlabel(\"Episode\")\n","    plt.plot(temp_episode, temp_minDist)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    for n in range(len(temp_score)-10):\n","        score_avg.append(np.mean(temp_score[10*n: 10*n+10]))\n","        reward_avg.append(np.mean(temp_reward[10*n: 10*n+10]))\n","\n","    plt.subplot(4,1,3)\n","    plt.title('Average game score of 10 episodes changes over time')\n","    plt.ylabel(\"Average game score\")\n","    plt.xlabel(\"Episode (x10)\")\n","    plt.plot(score_avg)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    plt.subplot(4,1,4)\n","    plt.title('Average emotioal reward of 10 episodes changes over time')\n","    plt.ylabel(\"Average emotional reward\")\n","    plt.xlabel(\"Episode (x10)\")\n","    plt.plot(reward_avg)\n","    plt.grid()\n","    print(\"\\n\\n\\n\")\n","    plt.show()\n","\n","    # Clarify the lists are global list before modify them\n","    global episode_array, epsilon_array, num_steps, rewards, gamescores, minAlien_arr, minDist_arr\n","\n","    episode_array = np.zeros(len(temp_epsilon))\n","    episode_array = temp_episode[:]\n","    epsilon_array = np.zeros(len(temp_epsilon))\n","    epsilon_array = temp_epsilon[:]\n","    num_steps = np.zeros(len(temp_epsilon))\n","    num_steps = temp_step[:]\n","    rewards = np.zeros(len(temp_epsilon))\n","    rewards = temp_reward[:]\n","    gamescores = np.zeros(len(temp_epsilon))\n","    gamescores = temp_score[:]\n","    minAlien_arr = np.zeros(len(temp_epsilon))\n","    minAlien_arr = temp_minAlien[:]\n","    minDist_arr = np.zeros(len(temp_epsilon))\n","    minDist_arr = temp_minDist[:]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3AX_X3K8etl","executionInfo":{"status":"aborted","timestamp":1622039511275,"user_tz":-480,"elapsed":56,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["if check_load_model == True:\n","  \n","    load_checkpoint()\n","    load_and_draw()\n","\n","    for episode in range(episode_array[-1]+1, 1500):\n","        # play\n","        # reward, epsilon, num_step, minAlienmat_avg, minDistmat_avg, score_per_epi\n","        reward, epsilon, steps, minAlienmat_avg, minDistmat_avg, score = play_and_record(agent, env, exp_replay, n_steps=10000,\n","                                                episode=episode, train_step=train_step, td_loss=td_loss)\n","        #print(\"Alien Distance:  \", minAlienmat_avg, \"; Egg Distance:  \", minDistmat_avg)\n","        # Store epsilon, scores and steps of each episode in epsilon_array\n","        epsilon_array.append(epsilon)\n","        num_steps.append(steps)\n","        rewards.append(reward)\n","        gamescores.append(score)\n","        minAlien_arr.append(minAlienmat_avg)\n","        minDist_arr.append(minDistmat_avg)\n","        \n","        # Create a CSV file to save output values. Save it and save training models every 10 episode. \n","        fileName = '/content/gdrive/My Drive/Colab Notebooks/Pure score.csv'\n","\n","        if episode % 5 == 0 or episode == 1499:\n","            save_checkpoint()\n","\n","            with open(fileName, mode='w') as csv_file:\n","                fieldnames = ['Episode', 'Epsilon', 'Step', 'GameScore', 'Reward', 'MinAlien', 'MinDist']\n","                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","                writer.writeheader()\n","\n","                for i in range(episode):\n","                    writer.writerow({'Episode': i, 'Epsilon': epsilon_array[i],\n","                                    'Step': num_steps[i], 'GameScore': gamescores[i], 'Reward': rewards[i], \n","                                     'MinAlien': minAlien_arr[i], 'MinDist': minDist_arr[i]})\n","\n","        # Adjust agent parameters by loading policy network into target network\n","        if episode % 30 == 0 and episode != 0:\n","            load_weigths_into_target_network(agent, target_network)\n","            print(\"Load nerual network weights from Policy Network to Target Nework\")\n","\n","            if np.mean(gamescores[-10:]) > 800:\n","                print('Should be ok')\n","                break\n","                \n","        print(\"episode = %i, buffer size = %i, epsilon = %.5f\" % (episode, len(exp_replay), epsilon))\n","        print(\"game score: \", score, \"  emotional reward: \", reward)\n","        clear_output(wait=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-R25uewZnERI","executionInfo":{"status":"aborted","timestamp":1622039511276,"user_tz":-480,"elapsed":56,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["rewards_evaluate = []\n","gameScores_evaluate = []\n","\n","def evaluate():\n","    fileName = '/content/gdrive/My Drive/Colab Notebooks/Pure score Evaluate.csv'\n","    load_checkpoint()\n","\n","    for episode in range(1500, 1600):\n","        reward, epsilon, steps, minAlienmat_avg, minDistmat_avg, gameScore = play_and_record(agent, env, exp_replay, n_steps=10000,\n","                                                episode=episode, train_step=train_step, td_loss=td_loss)\n","\n","        rewards_evaluate.append(reward)\n","        gameScores_evaluate.append(gameScore)\n","        print(\"At Episode \", episode-1500, \", Game Score is \", \n","              gameScore, \", emotional model Reward is \", reward) \n","\n","        with open(fileName, mode='w') as csv_file:\n","            fieldnames = ['Episode', 'GameScore', 'Reward']\n","            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","            writer.writeheader()\n","            \n","            for i in range(len(rewards_evaluate)):\n","                writer.writerow({'Episode': i, 'GameScore': gameScores_evaluate[i], 'Reward': rewards_evaluate[i]})  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_JkhLShzQWS","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1622039511278,"user_tz":-480,"elapsed":58,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["# Load and draw information from load models and saved .CSV file\n","load_and_draw()\n","\n","# Evaluate the training result (Deliberately put down a wrong punctuation ';' to pause the program)\n","\n","#evaluate();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwGJNxGZDJge","executionInfo":{"status":"aborted","timestamp":1622039511279,"user_tz":-480,"elapsed":59,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["with open('/content/gdrive/My Drive/Colab Notebooks/Pure score Evaluate.csv', mode='r') as csv_file:\n","        csv_reader = csv.DictReader(csv_file)\n","        line_count = 0 \n","\n","        temp_episode_evaluate = []\n","        temp_score_evaluate = []\n","        temp_reward_evaluate = []\n","\n","        for row in csv_reader:\n","            if line_count == 0:\n","                print(f'Column names are {\", \".join(row)}')\n","                line_count += 1\n","\n","            line_count += 1\n","            temp_episode_evaluate.append(int(row['Episode']))\n","            temp_score_evaluate.append(float(row['GameScore']))\n","            temp_reward_evaluate.append(float(row['Reward']))\n","\n","        plt.subplot(2,1,1)\n","        plt.title(\"Evaluated Game Scores\")\n","        plt.ylabel(\"Game Scores\")\n","        plt.xlabel(\"Episode\")\n","        plt.plot(temp_score_evaluate)\n","        plt.grid()\n","        plt.show()\n","\n","        plt.subplot(2,1,2)\n","        plt.title(\"Evaluated Emotional Model Rewards\")\n","        plt.ylabel(\"Rewards\")\n","        plt.xlabel(\"Episode\")\n","        plt.plot(temp_reward_evaluate)\n","        plt.grid()\n","        print(\"\\n\\n\\n\")\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjmeKJLSnESU"},"source":["### Make A Video"]},{"cell_type":"code","metadata":{"id":"vLlFTs_YnESU","executionInfo":{"status":"aborted","timestamp":1622039511280,"user_tz":-480,"elapsed":59,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["agent.epsilon=0 # To reset epsilon back to previous value if you want to go on training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOmyCz2Sn4Vg","executionInfo":{"status":"aborted","timestamp":1622039511285,"user_tz":-480,"elapsed":64,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["#!pip install 'imageio==2.4.0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOR2he_GjkHd","executionInfo":{"status":"aborted","timestamp":1622039511286,"user_tz":-480,"elapsed":64,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["import IPython\n","\n","def embed_mp4(filename):\n","  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n","  video = open(filename,'rb').read()\n","  b64 = base64.b64encode(video)\n","  tag = '''\n","  <video width=\"640\" height=\"480\" controls>\n","    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n","  Your browser does not support the video tag.\n","  </video>'''.format(b64.decode())\n","\n","  return IPython.display.HTML(tag)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zxzZIeRZE2f","executionInfo":{"status":"aborted","timestamp":1622039511287,"user_tz":-480,"elapsed":65,"user":{"displayName":"James Lu","photoUrl":"","userId":"08997324148147859027"}}},"source":["import imageio\n","from skimage import img_as_ubyte, img_as_float\n","\n","# Redefine evaluate class returning only four outputs. Do this for generating a video\n","def evaluate_video(env, agent, n_games, greedy=False, t_max=10000):\n","    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n","\n","    filename = \"/content/gdrive/My Drive/Colab Notebooks/MsPacman_v0\" + \".mp4\"\n","\n","    with imageio.get_writer(filename, fps=30) as video:\n","        for n in range(n_games):\n","            gameScore = 0\n","            reward = 0\n","\n","            while gameScore < 400 or gameScore > 700:\n","                gameScore = 0\n","                reward = 0\n","\n","                s = env.reset()\n","                temp_video_data = []\n","\n","                for step in range(t_max):  \n","                    qvalues = agent.get_qvalues([s])\n","                    action, epsilon = (qvalues.argmax(axis=-1)[0], 0) if greedy else agent.sample_actions(qvalues, 1300)\n","                    _, _, s, score, r, done, _ = env.step(action)\n","                    gameScore += score\n","                    reward += r\n","\n","                    render_img = env.render(mode='rgb_array')  \n","                    red_layer = render_img[:, :, 0]\n","                    green_layer = render_img[:, :, 1]\n","                    blue_layer = render_img[:, :, 2]\n","\n","                    temp_memory = np.zeros((210, 160, 3))\n","                    temp_memory[:, :, 0] = red_layer \n","                    temp_memory[:, :, 1] = green_layer \n","                    temp_memory[:, :, 2] = blue_layer\n","\n","                    # Convert data type of elements in the array from float64 to unit8\n","                    temp_memory = img_as_ubyte(temp_memory/255)\n","\n","                    temp_video_data.append(temp_memory)\n","\n","                    #video.append_data(render_img)\n","                    #video_temp.append_data(temp_memory)\n","\n","                    if done: \n","                        for i in range(80):\n","                            temp_video_data.append(temp_memory)  # To freeze the screen on the last slide\n","      \n","                        print(\"Game score: \", gameScore, \"   Reward: \", reward)\n","                        break\n","\n","            if gameScore >= 400:\n","                for index in range(len(temp_video_data)):\n","                    video.append_data(temp_video_data[index])\n","\n","    return embed_mp4(filename)\n","\n","   \n","\n","# Generate a recorded video\n","evaluate_video(make_env(), agent, n_games=5, greedy=False)\n"],"execution_count":null,"outputs":[]}]}